{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scaled dot production-1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2))/math.sqrt(d_k)\n",
    "    print(f\"scaled.size(): {scaled.size()}\")\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return attention, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaled dot production- 2nd time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is the scaled dot production attention mechanism used for?\n",
    "### breifly speaking, it's quite important features in transformer to weight the different part of input when generating the output\n",
    "## why scaled value is sqrt(d_qkv)?\n",
    "### 1.it's an experimental value, practically, this denominator can make the value to be in the normal ditribution after dot product\n",
    "### 2.the scaling prevent softmax value from too steep, which can happen due to the large value resulting from the dot product operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_production(q, k, v, mask=None):\n",
    "    d_qkv = q.shape[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1,-2))/torch.sqrt(d_qkv)\n",
    "    if mask:\n",
    "        scaled+=mask\n",
    "    attention = F.softmax(scaled)\n",
    "    value = torch.matmul(v, attention)\n",
    "    return attention, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model//num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model, 3*d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        bs, max_sequence_length, d_model = x.size()\n",
    "        print(f\"x.size: {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(bs, max_sequence_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q, k, v = qkv.chunk(3)\n",
    "        attention, values = scaled_dot_product(q, k, v, mask)\n",
    "        values = values.reshape(bs, max_sequence_length, self.num_heads*self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiAttention head-2nd time\n",
    "## why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_v2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 d_model,\n",
    "                 num_head,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.head_dim = d_model//num_head\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, max_seq, input_dim = x.shape\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(bs, max_seq, self.num_head, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        attn, value = scaled_dot_product(q, k, v)\n",
    "        # concatenate\n",
    "        value = value.reshape(bs, max_seq, self.num_head*self.d_model)\n",
    "        out = self.fc(value)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.8415,  0.0464,  0.0022,  0.5403,  0.9989,  1.0000],\n",
       "        [ 0.9093,  0.0927,  0.0043, -0.4161,  0.9957,  1.0000],\n",
       "        [ 0.1411,  0.1388,  0.0065, -0.9900,  0.9903,  1.0000],\n",
       "        [-0.7568,  0.1846,  0.0086, -0.6536,  0.9828,  1.0000],\n",
       "        [-0.9589,  0.2300,  0.0108,  0.2837,  0.9732,  0.9999],\n",
       "        [-0.2794,  0.2749,  0.0129,  0.9602,  0.9615,  0.9999],\n",
       "        [ 0.6570,  0.3192,  0.0151,  0.7539,  0.9477,  0.9999],\n",
       "        [ 0.9894,  0.3629,  0.0172, -0.1455,  0.9318,  0.9999],\n",
       "        [ 0.4121,  0.4057,  0.0194, -0.9111,  0.9140,  0.9998]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq):\n",
    "        super().__init__()\n",
    "        self.max_seq = max_seq\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        odd_i = torch.arange(1, self.d_model, 2).float()\n",
    "        even_denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        odd_denominator = torch.pow(10000, (odd_i-1)/self.d_model)\n",
    "        position = torch.arange(self.max_seq).reshape(self.max_seq, 1)\n",
    "        even_PE = torch.sin(position/even_denominator)\n",
    "        odd_PE = torch.cos(position/odd_denominator)\n",
    "        PE = torch.concat((even_PE, odd_PE), dim=1)\n",
    "        return PE\n",
    "    \n",
    "pe = PositionalEncoding(6,10)\n",
    "pe.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(atten, v)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m atten, value\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mmodules):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     13\u001b[0m                  input_dim,\n\u001b[0;32m     14\u001b[0m                  d_model,\n\u001b[0;32m     15\u001b[0m                  num_head,\n\u001b[0;32m     16\u001b[0m                  max_squence):\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n",
      "\u001b[1;31mTypeError\u001b[0m: module() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def scaled_dot_product(q,k,v):\n",
    "    \"\"\"\n",
    "    q.shape = b, maxsequence, head_dim\n",
    "    \"\"\"\n",
    "    q_d = q.shape[-1]\n",
    "    scaled_qk = torch.matmul(q, k.transpose(-1,-2))/math.sqrt(q_d)\n",
    "    atten = F.softmax(scaled_qk,dim=-1)\n",
    "    value = torch.matmul(atten, v)\n",
    "    return atten, value\n",
    "\n",
    "class MultiHeadAttention(nn.modules):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 d_model,\n",
    "                 num_head,\n",
    "                 max_squence):\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.head_dim = d_model/num_head\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, max_squence, input_dim = x.shape\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(bs, max_squence, self.num_head, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        atten, value = scaled_dot_product(q, k, v)\n",
    "        value = value.reshape(bs, max_squence, self.num_head*self.head_dim)\n",
    "        out = self.fc(value)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def PositionalEncode(x):\n",
    "    bs, max_seq, d_model = x.shape\n",
    "    postion = torch.arange(0, max_seq, 1)\n",
    "    even_i = torch.arange(0, d_model, 2)\n",
    "    odd_i = torch.arange(1, d_model, 2)\n",
    "    odd_denominator = torch.pow(10000, odd_i)\n",
    "    even_denominator = torch.pow(10000, even_i)\n",
    "    odd_PE = torch.sin(postion/odd_denominator)\n",
    "    even_PE = torch.cos(postion/even_denominator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 5, 16)\n",
    "bs, max_seq, d_model = x.shape\n",
    "postion = torch.arange(0, max_seq, 1)\n",
    "even_i = torch.arange(0, d_model, 2)\n",
    "odd_i = torch.arange(1, d_model, 2)\n",
    "odd_denominator = torch.pow(10000, odd_i/d_model)\n",
    "even_denominator = torch.pow(10000, even_i/d_model)\n",
    "postion = postion.reshape(max_seq, 1)\n",
    "odd_PE = torch.sin(postion/odd_denominator)\n",
    "even_PE = torch.cos(postion/even_denominator)\n",
    "PE = torch.concat([odd_PE, even_PE], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((max_seq,d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((3,4,5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.permute(0,2,1,3).reshape(3,5,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.reshape(3,5,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 24])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q,k,v):\n",
    "    q_d = q.shape[-1]\n",
    "    qk_scaled = torch.matmul(q, k.transpose(-1,-2))/math.sqrt(q_d)\n",
    "    atten = F.softmax(qk_scaled)\n",
    "    value = torch.matmul(atten, v)\n",
    "    return atten, value\n",
    "\n",
    "class MultiHeadAttention(nn.modules):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 d_model,\n",
    "                 max_sequence,\n",
    "                 num_head\n",
    "                 ):\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.head_dim = d_model/num_head\n",
    "\n",
    "        self.max_sequence = max_sequence\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3*self.d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, max_seq, input_dim = x.shape\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(bs, max_seq, self.num_head, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2,1,3)\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "        atten, value = scaled_dot_product(q, k, v)\n",
    "        value = qkv.permute(0,2, 1,3).reshape(bs,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-PT-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
